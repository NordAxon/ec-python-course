{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "celtic-internet",
   "metadata": {},
   "source": [
    "## Install huggingface if you don't have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-journey",
   "metadata": {},
   "source": [
    "# Question answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "antique-powder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlexanderHagelborn\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "express-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "  \n",
    "class QA:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "        \n",
    "    def answer_question(self, question, context):\n",
    "        return self.pipeline(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "derived-guinea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5675859451293945,\n",
       " 'start': 24,\n",
       " 'end': 49,\n",
       " 'answer': 'fifteen hundred years old'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = QA()\n",
    "question = 'tell me my age?'\n",
    "context = \"My name is alex and i'm fifteen hundred years old. I drive a scuba diver dog named patty to work every day. it's fast as hell let me tell you!\"\n",
    "\n",
    "qa.answer_question(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-hydrogen",
   "metadata": {},
   "source": [
    "# Text generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "emerging-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "class TextGenerator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline = pipeline(\"text-generation\", model='distilgpt2')\n",
    "        \n",
    "    def generate_text(self, context, min_length=50, max_length=500):\n",
    "        \" Generates text from the context\"\n",
    "        return self.pipeline(context, min_length=min_length, max_length=max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "inner-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "textpipe = pipeline(\"text-generation\", model='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "atmospheric-cocktail",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'hi my name is Michael, but the other two are the others.\\n\"Miles can be a beautiful guy - and I\\'m still feeling very sad.\\n\"But I\\'m just feeling like Michael is a good man,\" says Ms. Evans'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg = TextGenerator()\n",
    "tg.generate_text('hi my name', max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-convertible",
   "metadata": {},
   "source": [
    "# Sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adopted-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentimentAnalyser:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline = pipeline(\"sentiment-analysis\")\n",
    "                                 \n",
    "    def analyse_text(self, text):\n",
    "        return self.pipeline(text)\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "permanent-majority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SentimentAnalyser()\n",
    "sa.analyse_text('I hate you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "competent-feedback",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_expand_inputs_for_generation',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_decoder_start_token_id',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_pad_token_id',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_stopping_criteria',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_into_model',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_input_ids_for_generation',\n",
       " '_push_to_hub',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_version',\n",
       " 'add_memory_hooks',\n",
       " 'add_module',\n",
       " 'adjust_logits_during_generation',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'beam_sample',\n",
       " 'beam_search',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'classifier',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'distilbert',\n",
       " 'double',\n",
       " 'dropout',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'get_buffer',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'greedy_search',\n",
       " 'group_beam_search',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'is_parallelizable',\n",
       " 'load_state_dict',\n",
       " 'load_tf_weights',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_labels',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'pre_classifier',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'sample',\n",
       " 'save_pretrained',\n",
       " 'set_input_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sa.pipeline.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-absorption",
   "metadata": {},
   "source": [
    "# Efficientnet\n",
    "\n",
    "This doesn't work. The model has no classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "genuine-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from efficientnet_pytorch) (1.9.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "  Building wheel for efficientnet-pytorch (setup.py): started\n",
      "  Building wheel for efficientnet-pytorch (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=200aee346758d15f1cbd0fdbaa25823c05e0979dbc579e7e1e295ba11ac10e60\n",
      "  Stored in directory: c:\\users\\alexanderhagelborn\\appdata\\local\\pip\\cache\\wheels\\84\\b9\\90\\25a0195cf95fb5533db96f1c77ea3f296b7cc86ae8ae48e3dc\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "blessed-space",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to C:\\Users\\AlexanderHagelborn/.cache\\torch\\hub\\checkpoints\\efficientnet-b0-355c32eb.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fdc94c1b154d82a3f7e0f33d7c5f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/20.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "focal-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.rand(1, 3, 224, 224)\n",
    "model.eval()\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-wildlife",
   "metadata": {},
   "source": [
    "# CLIP for image classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adjusted-elizabeth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cae7b12921149f09ceef0a7cc1675a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0df9ec3dd564d4f9622c28b339d76b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abfead36b6a407b919ddc5378ce12f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e89f4c2a774b22898ef40a3f96b2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f663e0f4a55344fe85a86f10dd227cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c5a1224aa24c1d8f52d394dad06207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d61aa915784779b6f1ef9daa749ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9694ca535f4906a9f06a0ca5938c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "chronic-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class ImageClassifier:\n",
    "    \n",
    "    def __init__(self, labels = ['cat', 'dog', 'banana']):\n",
    "        self.labels = labels\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "    def classify(self, image_file):\n",
    "        image = Image.open(image_file)\n",
    "        inputs = processor(text=self.labels, images=image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "        probs = logits_per_image.softmax(dim=1).detach().numpy()[0] # we can take the softmax to get the label probabilities\n",
    "        return self._yield_output(probs, self.labels)\n",
    "        \n",
    "    def _yield_output(self, probs, labels):\n",
    "        \"Returns a dict mapping from label to probability\"\n",
    "        result = {z[0]:z[1] for z in zip(labels, probs)}\n",
    "        return result\n",
    "        \n",
    "    def change_labels(self, new_labels: []):\n",
    "        \"Changes the class labels that the model uses\"\n",
    "        self.labels = new_labels\n",
    "        \n",
    "    def get_labels(self):\n",
    "        \"Returns the class labels the model uses at inference\"\n",
    "        return self.labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abandoned-thriller",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cat': 0.98697644, 'dog': 0.01055414, 'fish': 0.002469376}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip = ImageClassifier(labels = ['cat', 'dog', 'fish'])\n",
    "clip.classify('cat.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
